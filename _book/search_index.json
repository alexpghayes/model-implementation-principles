[
["index.html", "Conventions for R Modeling Packages Chapter 1 Introduction", " Conventions for R Modeling Packages 2018-08-17 Chapter 1 Introduction The S language has had unofficial conventions for modeling function, such as: using formulas to specify model terms, variable roles, and some statistical calculations basic infrastructure for creating design matrices (model.matrix), computing statistical probabilities (logLik), and commom analyses (anova, p.adjust, etc.) existing OOP classes like predict, update, etc. Despite a note written by an R core member in 2003, these conventions have never been formally required for packages and this has led to a a significant amount of between- and within-package heterogeneity. For example, the table below shows myriad methods for obtaining class probability estimates for a set of modeling functions: Function Package Code lda MASS predict(obj) glm stats predict(obj, type = &quot;response&quot;) gbm gbm predict(obj, type = &quot;response&quot;, n.trees) mda mda predict(obj, type = &quot;posterior&quot;) rpart rpart predict(obj, type = &quot;prob&quot;) Weka RWeka predict(obj, type = &quot;probability&quot;) logitboost LogitBoost predict(obj, type = &quot;raw&quot;, nIter) pamr.train pamr pamr.predict(obj, type = &quot;posterior&quot;) Note that many use different values of type to obtain the same output and one does not use the standard S3 predict method for the function. Also notice that two models require an extra parameter to make predictions and despite these parameters signifying the same idea (the ensemble size), different parameter names are used. There are myriad examples of inconsistencies in function design, return values, and other aspects of modeling functions. The goal of this document is to define a specification for creating functions and packages for new modeling packages. These are opinionated specifications but are meant to reflect reasonable positions for standards based on prior experience. A number of these guidelines are specific to the tidyverse (e.g. “Function names should use snake_case instead of camelCase.”). However, the majority are driven by common sense and good design principles (e.g. “All functions must be reproducible from run-to-run.”). The chapters in this document contain recommendations for different aspects of modeling functions. The items within are meant to be succinct and concise. In many cases, further details can be found in links to the Notes chapter. Examples and implementation details can be found there. "],
["general-conventions.html", "Chapter 2 General Conventions", " Chapter 2 General Conventions Function names should use snake_case instead of camelCase. All functions must be reproducible from run-to-run. If there is a strong need for producing output during execution, there should be a verbose option that defaults to no output. {notes} When parallelism is used: Provide an argument to specify the amount (e.g. number of cores if appropriate) and default the function to run sequentially. Computations should be easily reproducible, even when run in parallel. Parallelism should not be an excuse for irreproducibility. "],
["function-interfaces.html", "Chapter 3 Function Interfaces 3.1 User-Facing Functions 3.2 Computational Functions", " Chapter 3 Function Interfaces We distinguish between “top-level”/“user-facing” api’s and “low-level”/“computational” api’s. The former being the interface between the users of the function (with their needs) and the code that does the estimation/training activities. When creating model objects, the computational code that fits the model should be decoupled from the interface code to specify the model. {note} 3.1 User-Facing Functions At a minimum, the top-level model function should be a generic with methods for data frames, formulas, and possibly recipes. These methods error trap bad arguments, format/encode the data, them pass it along to the lower-level computational code. Do not require users to create dummy variables from their categorical predictors. Provide a formula and/or recipe interface to your model to do this (see the next item) and other methods should error trap if qualitative data should not be subsequently used. Only user-appropriate data structures should be accommodated for the user-facing function. The underlying computational code should make appropriate transformations to computationally appropriate formats/encodings. {note} Design the top-level code for humans. This includes using sensible defaults and protecting against common errors. Design your top-level interface code so that people will not hate you. {note} Parameters that users will commonly modify should be main arguments to the top-level function. Others, especially those that control computational aspects of the fit, should be contained in a control object. If your model passes ... to another modeling function, consider the names of your top-level function arguments to avoid conflicts with the argument names of the underlying function. {note} Common arguments should use standardized names. {note} If the call object is saved, it should be checked for size. In cases where the function is invoked by do.call(&quot;foo&quot;, bar), the data set may be embedded in the argument list bar. 3.2 Computational Functions A test set should never be required when fitting a model. If internal resampling is involved in the fitting process, there is a strong preference for using tidymodels infrastructure so that a common interface (and set of choices) can be used. If this cannot be done (e.g. the resampling occurs in C code), there should be some ability to pass in integer values that define the resamples. In this way, the internal sampling is reproducible. The same is true for other infrastructure (e.g. yardstick for performance measures, etc.) When possible, do not reimplement computations that have been done well elsewhere (tidy or not). For example, kernel methods should use the infrastructure in kernlab, exponential family distribution computations should use those in ?family etc. For modeling packages that use random numbers, setting the seed in R should control how random numbers are generated internally. At worst, a random number seed for non-R code (e.g. C, Java) should be an argument to the main modeling function. Computational code should (almost) always use X[ , ,drop = FALSE] when subsetting matrices (and non-tibble data frames) to make sure that the 2D structure is maintained. Using the call object for post-estimation activies is discouraged. {note} "],
["print-and-summary-methods.html", "Chapter 4 Print and Summary Methods", " Chapter 4 Print and Summary Methods Every class should have a print methods that gives a concise description of the object. The print method should invisibly return the original object. The number of significant digits should be an option and should use the global default. Printing the call is optional. summary methods are helpful but required. These should create more verbose descriptions of the object. the print method for the summary object should follow the bullet-pointed remarks above for printing. "],
["model-predictions.html", "Chapter 5 Model Predictions 5.1 Input Data 5.2 Return Values", " Chapter 5 Model Predictions The function to produce predictions should be a class-specific predict method with arguments object newdata, and possibly type. Other arguments, such as level, should be standardized. {note} The main predict method can internally defer to separate, unexported functions (predict_class, etc). type should also come from a set of pre-defined values such as type application response numeric predictions class hard class predictions prob class probabilities, survivor probabilities link glm linear predictor conf_int confidence intervals pred_int prediction intervals raw direct access to prediction function param_pred predictions across tuning parameters Other values should be assigned with consensus. 5.1 Input Data If newdata is not supplied, an error should be thrown. The model outcome should never be required to be in newdata. newdata should be tolerant of extra columns. For example, if all variables are in some data frame dataset, predict(object, dataset) should immediately know which variables are required for prediction, check for their presence, and select only those from dataset before proceeding. The prediction code should work whether newdata has multiple rows or a single row. 5.2 Return Values By default, newdata should not be returned by the prediction function. The return value is a tibble with the same number of rows as the data being predicted. This implies that na.action should not affect the dimensions of the outcome object. The class of the tibble can be overloaded to accommodate specialized methods as long as back data frame functionality is maintained. {note} The return tibble can contain extra attributes for values relevant to the prediction (e.g. level for intervals) but care should be taken to make sure that these attributes are not destroyed when standard operations are applied to the tibble (e.g. arrange, filter, etc.). Columns of constant values (e.g. adding level as a column) should be avoided. Specific cases: For univariate, numeric point estimates, the column should be named .pred. For multivariate numeric predictions (excluding probabilities), the columns should be named .pred_{outcome name}. Class predictions should be factors with the same levels as the original outcome and named .pred_class. For class probability predictions, the columns should be named the same as the factor levels, e.g., .pred_{level}. If interval estimates are produced (e.g. prediction/confidence/credible), the column names should be .pred_lower and .pred_upper. If a standard error is produced, it should be named .std_error. For predictions that are not simple scalars, such as distributions or non-rectangular structures, the .pred column should be a list-column {note} "],
["standardized-argument-names.html", "Chapter 6 Standardized Argument Names 6.1 Statistical and Tuning Parameters 6.2 Data Arguments 6.3 Numerical Arguments", " Chapter 6 Standardized Argument Names Dot usage: If there is a possability of arugment name conflicts between the function and any arguments passed down through ..., it is strongly suggested that the argument names be prefixed with a dot (e.g. .data, .x, etc.) 6.1 Statistical and Tuning Parameters direction for the type of hypothesis test alternative. level for interval levels (e.g., confidence, credible, prediction, and so on). 6.2 Data Arguments x for predictors or generic data objects. y for outcomes. weights for case weights. newdata for data to be predicted. na_rm for missing data handling. 6.3 Numerical Arguments times for the number of bootstraps, simulations, "],
["notes.html", "Chapter 7 Notes", " Chapter 7 Notes writing output To write messages, cat() or message() can be used. Their differences: cat() goes to standard out and message() goes to standard error. When used inside of some parallel processing backends, cat() output is obscured inside of the RStudio IDE. Unless coded otherwise, cat() uses the usual formatting whereas the default formatting for message() is different. For this code: library(emojifont) message(&quot;Help! I&#39;m trapped in the well!!&quot;) cat(&quot;No, you&#39;re not&quot;, emoji(&#39;roll_eyes&#39;), &quot;\\n&quot;) the Rstudio IDE output is: and basic terminal output is: This post may also be helpful in deciding. ↩️ decoupling functions For example, for some modeling method, the function foo would be the top-level api that users experience and some other function (say compute_foo_fit) is used to do the computations. This allows for different interfaces to be used to specify the model that pass common data structures to compute_foo_fit. ↩️ appropriate data structures For example: Categorical data should be in factor variables (as opposed to binary or integer representations). Rectangular data structures that allows for mixed data types should always be used even when the envisioned application would only make sense when the data are single type. For strictly univariate response models, vector input is acceptable for the outcome argument. Censored data should follow the survival::Surv convention. ↩️ top-level design examples For example: Suppose a model can only fit numeric or two-class outcomes and uses maximum likelihood. Instead of providing the user with a distribution option that is either “Gaussian” or “Binomial”, determine this from the type of the data object (numeric or factor) and set internally. This prevents the user from making a mistake that could haven been avoided. If a model parameter is bounded by some aspect of the data, such as the number of rows or columns, coerce bad values to this range (e.g. mtry = min(mtry, ncol(x))) with an accompanying warning when this is critical information. ↩️ avoid common parameter names For example, control is the type of argument used in many functions so have a function specific argument (e.g. foo_control) is advisable in these situations. ↩️ standardize names For example, many functions use some variation of level for confidence levels (as opposed to alpha). The names in Chapter 6 are preferable for the specific context. ↩️ discussing new names A good venue for this discussion is RStudio Community ↩️ avoid computing on call objects Historically, the call object that results from a model fit was considered a good method to capture details of the existing model fit. This object could be parsed and manipulated for the purpose of continuing or revising the model between function calls. This can be problematic because the call object is relative to the environment and the call itself is agnostic to its original environment. If the call is changed and re-evaluated, the current environment may be inappropriate. This could result in errors due to the required objects not being exposed in the current environment. Note that the internals of common modeling functions, such as lm, do exactly this. However, these manipulations occur within the function call to that the environment is the same. ↩️ extra tibble classes Adding a new class to a tibble object might cause subsequent errors to dplyr operations. This code is a good example for how to maintain dplyr compatibility. ↩️ list column output Some examples: posterior distirbutions If a posterior distribution is returned for each sample, each element of the list column can be a tibble with as many rows as samples in the distribution. multiple hyperparameters When a predict method produces values over multiple tuning parameter values (e.g. glmnet), the list column elements have rows for every tuning parameter value combination that is being changed (e.g. lambda in glmnet). survivor probability predictions In time to event models, where survivor probabilities are produced for values of time, the return tibble has a column for .time. Other column names should conform to the standards here (e.g. .pred_lower if intervals are also returned). As an example from ?flexsurv::flexsurvreg: library(flexsurv) data(ovarian) fitg &lt;- flexsurvreg(formula = Surv(futime, fustat) ~ age, data = ovarian, dist = &quot;gengamma&quot;) For each new sample, this model can make probabilistic predictions at a number of user-specified time points. preds &lt;- summary(fitg, newdata = ovarian[1:2, &quot;age&quot;, drop = FALSE], t = c(100, 200, 300)) preds ## age=72.3315 ## time est lcl ucl ## 1 100 0.7988680 0.47174907 0.9656529 ## 2 200 0.4849180 0.12351454 0.7913514 ## 3 300 0.2784226 0.01015872 0.6418956 ## ## age=74.4932 ## time est lcl ucl ## 1 100 0.7278338 0.3558990811 0.9435343 ## 2 200 0.3856737 0.0458476743 0.7417648 ## 3 300 0.1964611 0.0003631246 0.6023775 A list of data frames is not very helpful. Better would be a data frame with a list column, were every element is a data frame with the predictions in the tidy format: tidy_preds ## # A tibble: 2 x 1 ## .pred ## &lt;list&gt; ## 1 &lt;tibble [3 × 4]&gt; ## 2 &lt;tibble [3 × 4]&gt; # predictions on the first subject at 3 time points tidy_preds$.pred[[1]] ## # A tibble: 3 x 4 ## .time .pred .pred_lower .pred_upper ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 100 0.799 0.472 0.966 ## 2 200 0.485 0.124 0.791 ## 3 300 0.278 0.0102 0.642 If a single data frame is needed, it is easy to make the conversion: tidyr::unnest(tidy_preds) ## # A tibble: 6 x 4 ## .time .pred .pred_lower .pred_upper ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 100 0.799 0.472 0.966 ## 2 200 0.485 0.124 0.791 ## 3 300 0.278 0.0102 0.642 ## 4 100 0.728 0.356 0.944 ## 5 200 0.386 0.0458 0.742 ## 6 300 0.196 0.000363 0.602 percentile predictions When using a quantile regression, one might make the median the default that is predicted. If multiple percentiles are requested, then .pred would be a tibble with a column for the predictions and another for the percentile. ↩️ "]
]
